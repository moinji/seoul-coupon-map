# utils/analysis_sungdong.py

import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# ë¶„ì„/ì‹œê°í™” í•¨ìˆ˜
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from wordcloud import WordCloud
import numpy as np
from scipy.stats import entropy
import os

def crawl_shops_sungdong(output_path='shops_sungdong.csv', max_pages=2):
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--window-size=1920,1080')

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)

    base_url = "https://www.sd.go.kr/main/webRecoveryCouponList.do?searchName=&searchEmdNm=&searchAddress=&searchBizRegNo=&key=5269&pageIndex={}"

    result_list = []

    try:
        for page in range(1, max_pages + 1):
            print(f"[INFO] í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘...")
            driver.get(base_url.format(page))

            # í…Œì´ë¸” ë¡œë”© ëŒ€ê¸°
            WebDriverWait(driver, 10).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, "table.table tbody tr"))
            )

            rows = driver.find_elements(By.CSS_SELECTOR, "table.table tbody tr")
            for row in rows:
                cols = row.find_elements(By.TAG_NAME, "th")
                if len(cols) < 3:
                    continue
                store = {
                    "store_name": cols[0].text.strip(),
                    "dong": cols[1].text.strip(),
                    "address": cols[2].text.strip()
                }
                result_list.append(store)

            time.sleep(0.8)

    except Exception as e:
        print(f"[ERROR] í¬ë¡¤ë§ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")

    finally:
        driver.quit()

    df = pd.DataFrame(result_list)
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"[SUCCESS] ì´ {len(df)}ê±´ ì €ì¥ ì™„ë£Œ: {output_path}")
    return df


def run_sungdong_analysis():
    import streamlit as st

    st.subheader("ğŸ¬ ì„±ë™êµ¬ì²­ ì†Œë¹„ì¿ í° ê°€ë§¹ì  ë°ì´í„° í¬ë¡¤ë§")
    max_pages = st.number_input("í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜", min_value=1, max_value=1681, value=2)

    if st.button("ğŸ•¸ï¸ ë°ì´í„° í¬ë¡¤ë§ ì‹¤í–‰"):
        with st.spinner("ë°ì´í„° ìˆ˜ì§‘ ì¤‘..."):
            df = crawl_shops_sungdong(max_pages=max_pages)
            st.success(f"âœ… ì´ {len(df)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
            st.dataframe(df)

            csv = df.to_csv(index=False, encoding='utf-8-sig')
            st.download_button("ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ", data=csv, file_name="shops_sungdong.csv", mime="text/csv")


    st.subheader("ğŸ“Š ì„±ë™êµ¬ ì†Œë¹„ì¿ í° ê°€ë§¹ì  ë¶„ì„")

    # ë°ì´í„° ë¡œë“œ
    csv_path = "shops_sungdong.csv"
    if not os.path.exists(csv_path):
        st.warning("CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•´ì£¼ì„¸ìš”.")
        return

    df = pd.read_csv(csv_path)

    st.markdown("### ğŸ“‹ ì›ë³¸ ë°ì´í„°")
    st.dataframe(df.head(10))

    st.markdown("### ğŸ“ˆ ì†Œì¬ì§€(ë™)ë³„ ê°€ë§¹ì  ìˆ˜")
    dong_counts = df["dong"].value_counts().sort_values(ascending=True)

    fig1, ax1 = plt.subplots(figsize=(10, 6))
    dong_counts.plot(kind="barh", ax=ax1)
    ax1.set_title("ì†Œì¬ì§€ë³„ ê°€ë§¹ì  ìˆ˜")
    ax1.set_xlabel("ê°€ë§¹ì  ìˆ˜")
    st.pyplot(fig1)

    st.markdown("### ğŸ“Œ ì†Œì¬ì§€ ë‹¤ì–‘ì„± ì§€ìˆ˜ (Entropy)")
    p = dong_counts / dong_counts.sum()
    diversity_score = entropy(p)
    st.metric("ğŸ§  ì†Œì¬ì§€ ë‹¤ì–‘ì„± (Shannon entropy)", f"{diversity_score:.3f}")